✅ работа с timestamp в Kafka -- либо producer их обрабатывает, либо Kafka
✅ topic syncing (для дальнейших трансформаций: join, etc.)

**Watermarks** -- специальные маркеры, которые указывают, до какого времени
данные в потоке гарантированно обработаны. Фактически просто задается допуск по
времени задержки данных (если больше этого допуска -- можно добавить еще
какую-то логику на обработку таких данных)

# Задача "максимум"

Собираем данные в реальном времени из нескольких продюсеров и обрабатываем их
через Flink SQL для расчета рекомендованной стоимости поездки в различных
регионах. Затем данные -> Kafka и сохраняться в ClickHouse для аналитики,
включая построение heatmap с ценами по регионам. 

upd: Использование ClickHouse избыточно, т.к. Flink по факту и является аналтическим инструментом.

# Выбор (реалистичность против удобства)

1. Обработка статичных данных (из файла на диске), но они идут без задержек в Kafka, потом в Flink.

Pros:
 - очень просто в реализации, ничего придумывать не надо.

Cons:
 - Теряется смысл симуляции. 

✅ 2. Симуляция реального времени (с учетом `timestamp`)

# структура "концовки"

... -> Flink -> Kafka -> Graphana

upd: ..... -> Kafka -> Go heatmap
updupd: -> Kafka -> Clickhouse -> Streamlit heatmap

https://pro.yandex.ru/ru-ru/moskva/knowledge-base/taxi/income-diff
https://www.oreilly.com/content/applying-the-kappa-architecture-in-the-telco-industry/
https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/event-time/generating_watermarks/
https://github.com/ververica/flink-sql-cookbook/tree/main
https://github.com/ververica/flink-sql-cookbook/blob/main/joins/03_kafka_join/03_kafka_join.md
https://github.com/ververica/flink-sql-cookbook/blob/main/aggregations-and-analytics/02_watermarks/02_watermarks.md
https://github.com/ververica/flink-sql-cookbook/blob/main/other-builtin-functions/03_current_watermark/03_current_watermark.md
https://nightlies.apache.org/flink/flink-docs-master/docs/concepts/time/

# how to work with watermarks
https://archive.is/6PMeB

https://www.confluent.io/blog/getting-started-with-apache-flink-sql/
https://www.confluent.io/blog/apache-flink-for-stream-processing/

#SQL
↓ !!!ONLY works with Flink ver. >= 1.15!!! ↓
https://aiven.io/blog/preview-JSON-SQL-functions-apache-flink-1.15.0#define-the-apache-flink-table
↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑
upd: check right ver. in /flink/jobs/job.sql

TODO add plots: orders time distr.; general taxi efficiency (
    => add jobs for this plots
TODO JOB-a rework
TODO scheme rework
TODO plt -> PlotlyJS (?)
TODO tidy docs
✅ web-app -> container
✅ update config in sake of full automation

docker-compose build && docker-compose up:
0 days, 00 hours, 04 minutes, and 16 seconds

docker-compose up:
0 days, 00 hours, 03 minutes, and 05 seconds

# WINDOW JOIN
https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/dev/table/sql/queries/window-join/
https://archive.is/8O309
